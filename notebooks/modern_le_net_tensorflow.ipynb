{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modern-le-net-tensorflow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmcenta/le-net-beginner/blob/master/notebooks/modern_le_net_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhfndcopF05z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTTIF3BhGE3J",
        "colab_type": "text"
      },
      "source": [
        "# Modern LeNet in TensorFlow\n",
        "This notebook is an implementation of the \"Hello World\" of ConvNets: the LeNet 5 architecture, evaluated on the MNIST dataset.\n",
        "\n",
        "This is intended as a tool for beginners to get familiar with TensorFlow as well as a training exercise for myself.\n",
        "\n",
        "This notebooks draws heavily from the TensorFlow turorials in style and topic order.\n",
        "\n",
        "Now that the introduction is out of the way, let's build our network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iutORH7hGgsy",
        "colab_type": "text"
      },
      "source": [
        "## Load the data\n",
        "Let's load the MNIST dataset from torchvision. We will also apply a transformation pipeline that converts the images to tensors and normalizes them to the [-1.0, 1.0] range.\n",
        "\n",
        "First, we import the necessary modules and classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1yoRT5hGf_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "97c2229b-bb4e-44d0-88ca-088d2b390e89"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNwTz-fAJClr",
        "colab_type": "text"
      },
      "source": [
        "Then we load the MNIST dataset and prepare it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JqFRS6K07jJs",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = (x_train / 255.0), x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY6Dr8E9KF7v",
        "colab_type": "text"
      },
      "source": [
        "Finally, we shuffle and batch the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecya8tTVKJ8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train)).shuffle(10000).batch(64)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bh2H09ZKYuq",
        "colab_type": "text"
      },
      "source": [
        "## Define the model\n",
        "\n",
        "Next we need to define the model we'll be using. We will be using a convolutional neural network deeply inspired on the classic LeNet architecture proposed in the \"Gradient-based learning applied to document recognition\" paper by LeCun et al."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLITzM-KkkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
        "\n",
        "class ModernLeNet(Model):\n",
        "  def __init__(self):\n",
        "    super(ModernLeNet, self).__init__()\n",
        "    # Convolutional layers\n",
        "    self.conv1 = Conv2D(6, 5, padding='same', activation='relu')\n",
        "    self.maxpool1 = MaxPool2D(2)\n",
        "    self.conv2 = Conv2D(16, 5, padding='same', activation='relu')\n",
        "    self.maxpool2 = MaxPool2D(2)\n",
        "\n",
        "    # Fully connected layers\n",
        "    self.flatten = Flatten()\n",
        "    self.fc1 = Dense(120, activation='relu')\n",
        "    self.fc2 = Dense(84, activation='relu')\n",
        "    self.fc3 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.maxpool2(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    return self.fc3(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "model = ModernLeNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A-7SN5fYffk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We will use the cross entropy loss for this clasification task, as well as the Adam optimizer with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JGzCZ_qYk8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ib2VRZNY8P_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}